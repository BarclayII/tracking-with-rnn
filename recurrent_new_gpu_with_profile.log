Using gpu device 1: GeForce GTX TITAN X (CNMeM is disabled, CuDNN 3007)
Using CUDNN instead of Theano conv2d
5776
Initializing parameters
Building network
float32
float32
/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gof/vm.py:818: UserWarning: CVM does not support memory profile, using Stack VM.
  'CVM does not support memory profile, using Stack VM.')
Building optimizer
Generating dataset
START
/usr/local/lib/python2.7/dist-packages/scipy/ndimage/interpolation.py:549: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.
  "the returned array has changed.", UserWarning)
Batch generation: 0.292640924454
Error allocating 576000000 bytes of device memory (out of memory). Driver report 228298752 bytes free and 12884705280 bytes total 
Traceback (most recent call last):
  File "recurrent_mlp_gpu.py", line 523, in <module>
    cost, bbox_seq, att_seq, mask = train(_len, data[:, :], label[:, 0, :], att, label[:, :])
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gof/vm.py", line 493, in __call__
    storage_map=storage_map)
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gof/link.py", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gof/vm.py", line 462, in __call__
    _, dt = self.run_thunk_of_node(current_apply)
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gof/vm.py", line 380, in run_thunk_of_node
    rval = self.thunks[idx]()
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/scan_module/scan_op.py", line 951, in rval
    r = p(n, [x[0] for x in i], o)
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/scan_module/scan_op.py", line 940, in <lambda>
    self, node)
  File "theano/scan_module/scan_perform.pyx", line 415, in theano.scan_module.scan_perform.perform (/home/gq/.theano/compiledir_Linux-3.16--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/scan_perform/mod.cpp:4409)
  File "theano/scan_module/scan_perform.pyx", line 397, in theano.scan_module.scan_perform.perform (/home/gq/.theano/compiledir_Linux-3.16--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/scan_perform/mod.cpp:4193)
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gof/vm.py", line 493, in __call__
    storage_map=storage_map)
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gof/link.py", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gof/vm.py", line 462, in __call__
    _, dt = self.run_thunk_of_node(current_apply)
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gof/vm.py", line 380, in run_thunk_of_node
    rval = self.thunks[idx]()
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gof/op.py", line 883, in rval
    fill_storage()
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gof/cc.py", line 1707, in __call__
    reraise(exc_type, exc_value, exc_trace)
  File "<string>", line 2, in reraise
MemoryError: Error allocating 576000000 bytes of device memory (out of memory).
Apply node that caused the error: GpuElemwise{mul,no_inplace}(GpuElemwise{Composite{maximum(i0, (i1 - Abs((i2 - i3))))},no_inplace}.0, GpuDimShuffle{0,1,2,x,3,4}.0)
Toposort index: 527
Inputs types: [CudaNdarrayType(float32, (False, False, True, False, False, True)), CudaNdarrayType(float32, (False, False, False, True, False, False))]
Inputs shapes: [(16, 9, 1, 100, 100, 1), (16, 9, 1, 1, 100, 100)]
Inputs strides: [(90000, 10000, 0, 100, 1, 0), (90000, 10000, 0, 0, 100, 1)]
Inputs values: ['not shown', 'not shown']
Outputs clients: [[GpuReshape{3}(GpuElemwise{mul,no_inplace}.0, MakeVector{dtype='int64'}.0)]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
Apply node that caused the error: for{inplace{1,2,3,4,5,6,7,8,9,10,11,12,13,14,},gpu,grad_of_scan_fn}(Shape_i{1}.0, GpuDimShuffle{0,1,x,2}.0, GpuDimShuffle{0,1,x,2,3}.0, GpuDimShuffle{0,x,1,2,3}.0, GpuDimShuffle{0,x,1,2,3}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, Elemwise{mul,no_inplace}.0, Elemwise{Mul}[(0, 1)].0, Elemwise{mul,no_inplace}.0, Elemwise{mul,no_inplace}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuElemwise{Composite{(i0 - sqr(i1))},no_inplace}.0, GpuDimShuffle{0,1,x,2,3,4,x}.0, GpuAlloc{memset_0=True}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{int64:int64:int64}.0, GpuSubtensor{::int64}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, Shape_i{1}.0, Shape_i{1}.0, Shape_i{1}.0, Shape_i{1}.0, Shape_i{1}.0, Shape_i{1}.0, conv1_filters, Wz, Uz, Wg, Wr, Ur, Ug, GpuDimShuffle{1,0}.0, GpuDimShuffle{x,0}.0, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0, GpuDimShuffle{x,0}.0, GpuDimShuffle{1,0}.0, GpuDimShuffle{x,0}.0, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0, GpuDimShuffle{1,0}.0, MakeVector{dtype='int64'}.0, Shape_i{0}.0, Shape_i{3}.0, Shape_i{2}.0)
Toposort index: 395
Inputs types: [TensorType(int64, scalar), CudaNdarrayType(float32, (False, False, True, False)), CudaNdarrayType(float32, (False, False, True, False, False)), CudaNdarrayType(float32, (False, True, False, False, False)), CudaNdarrayType(float32, (False, True, False, False, False)), CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, 4D), TensorType(int32, vector), TensorType(int32, vector), TensorType(int32, vector), TensorType(int32, vector), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, (False, False, True, False)), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, (False, False, True, False, False, False, True)), CudaNdarrayType(float32, vector), CudaNdarrayType(float32, 5D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, vector), CudaNdarrayType(float32, 5D), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar), CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, row), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, row), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, row), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), TensorType(int64, vector), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]
Inputs shapes: [(), (20, 16, 1, 4), (20, 16, 1, 5776, 1), (20, 1, 16, 5776, 1), (20, 1, 16, 1, 1), (20, 16, 160, 5776), (20, 16, 180, 5776), (20,), (20,), (20,), (20,), (20, 16, 3), (20, 16, 1, 4), (20, 16, 4), (20, 16, 1, 1, 100, 100, 1), (20,), (20, 16, 1, 100, 100), (20, 16, 4), (20, 16, 200), (20, 16, 1, 1), (20, 16, 4), (20, 16, 1, 1), (20, 16, 180, 5776), (20, 16, 160, 5776), (20, 16, 4), (21, 16, 4), (21, 16, 3), (21, 16, 200), (21, 16, 1, 1), (21, 16, 4), (21, 16, 5776, 1), (21, 16, 1, 1), (21, 16, 180, 5776), (21, 16, 160, 5776), (21,), (2, 16, 1, 10, 10), (2, 200), (2, 200), (2, 200), (2, 4), (), (), (), (), (), (), (16, 1, 10, 10), (5785, 200), (200, 200), (5785, 200), (5785, 200), (200, 200), (200, 200), (200, 5785), (1, 200), (200, 200), (200, 200), (1, 200), (200, 5785), (1, 200), (200, 200), (200, 5785), (3, 200), (4, 200), (4,), (), (), ()]
Inputs strides: [(), (-64, 4, 0, 1), (-92416, 5776, 0, 1, 0), (-92416, 0, 5776, 1, 0), (-16, 0, 1, 0, 0), (14786560, 924160, 5776, 1), (16634880, 1039680, 5776, 1), (4,), (-4,), (4,), (4,), (48, 3, 1), (64, 4, 0, 1), (64, 4, 1), (-10000, 200000, 0, 0, 100, 1, 0), (1,), (-10000, 200000, 0, 100, 1), (-64, 4, 1), (-3200, 200, 1), (-16, 1, 0, 0), (-64, 4, 1), (-16, 1, 0, 0), (-16634880, 1039680, 5776, 1), (-14786560, 924160, 5776, 1), (-64, 4, 1), (-64, 4, 1), (48, 3, 1), (3200, 200, 1), (16, 1, 0, 0), (64, 4, 1), (92416, 5776, 1, 0), (16, 1, 0, 0), (16634880, 1039680, 5776, 1), (14786560, 924160, 5776, 1), (1,), (1600, 100, 0, 10, 1), (200, 1), (200, 1), (200, 1), (4, 1), (), (), (), (), (), (), (100, 0, 10, 1), (200, 1), (200, 1), (200, 1), (200, 1), (200, 1), (200, 1), (1, 200), (0, 1), (1, 200), (1, 200), (0, 1), (1, 200), (0, 1), (1, 200), (1, 200), (1, 3), (1, 4), (8,), (), (), ()]
Inputs values: [array(20), 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', array(20), array(20), array(20), array(20), array(20), array(20), 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', array([16,  1, 10, 10]), array(16), array(10), array(10)]
Outputs clients: [[], [], [], [], [], [], [], [], [], [], [GpuSubtensor{int64}(for{inplace{1,2,3,4,5,6,7,8,9,10,11,12,13,14,},gpu,grad_of_scan_fn}.10, ScalarFromTensor.0)], [GpuSubtensor{int64}(for{inplace{1,2,3,4,5,6,7,8,9,10,11,12,13,14,},gpu,grad_of_scan_fn}.11, ScalarFromTensor.0)], [GpuSubtensor{int64}(for{inplace{1,2,3,4,5,6,7,8,9,10,11,12,13,14,},gpu,grad_of_scan_fn}.12, ScalarFromTensor.0)], [GpuSubtensor{int64}(for{inplace{1,2,3,4,5,6,7,8,9,10,11,12,13,14,},gpu,grad_of_scan_fn}.13, ScalarFromTensor.0)], [GpuSubtensor{int64}(for{inplace{1,2,3,4,5,6,7,8,9,10,11,12,13,14,},gpu,grad_of_scan_fn}.14, ScalarFromTensor.0)], [GpuReshape{2}(for{inplace{1,2,3,4,5,6,7,8,9,10,11,12,13,14,},gpu,grad_of_scan_fn}.15, MakeVector{dtype='int64'}.0)], [GpuDimShuffle{1,0,2}(for{inplace{1,2,3,4,5,6,7,8,9,10,11,12,13,14,},gpu,grad_of_scan_fn}.16)], [GpuReshape{2}(for{inplace{1,2,3,4,5,6,7,8,9,10,11,12,13,14,},gpu,grad_of_scan_fn}.17, MakeVector{dtype='int64'}.0)], [GpuDimShuffle{1,0,2}(for{inplace{1,2,3,4,5,6,7,8,9,10,11,12,13,14,},gpu,grad_of_scan_fn}.18)], [GpuReshape{2}(for{inplace{1,2,3,4,5,6,7,8,9,10,11,12,13,14,},gpu,grad_of_scan_fn}.19, MakeVector{dtype='int64'}.0)], [GpuReshape{2}(for{inplace{1,2,3,4,5,6,7,8,9,10,11,12,13,14,},gpu,grad_of_scan_fn}.20, MakeVector{dtype='int64'}.0)]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  Sorry, no tip for today.
Function profiling
==================
  Message: recurrent_mlp_gpu.py:469
  Time in 0 calls to Function.__call__: 0.000000e+00s
  Total compile time: 2.593547e+02s
    Number of Apply nodes: 473
    Theano Optimizer time: 1.585174e+02s
       Theano validate time: 2.283628e+00s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.007463e+02s
       Import time 3.035386e-01s

Time in all call to theano.grad() 1.141292e+01s
Time since theano import 430.378s
Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  Sorry, no tip for today.

Scan Op profiling ( scan_fn )
==================
  Message: None
  Time in 1 calls of the op (for a total of 20 steps) 1.442259e+02s

  Total time spent in calling the VM 1.441980e+02s (99.981%)
  Total overhead (computing slices..) 2.788377e-02s (0.019%)

Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  97.3%    97.3%     140.122s       7.01e+00s     Py      20       1   theano.scan_module.scan_op.Scan
   2.1%    99.5%       3.095s       1.07e-03s     C     2900     145   theano.sandbox.cuda.basic_ops.GpuElemwise
   0.3%    99.8%       0.427s       1.78e-03s     C      240      12   theano.sandbox.cuda.basic_ops.GpuCAReduce
   0.2%    99.9%       0.228s       2.85e-04s     C      800      40   theano.sandbox.cuda.basic_ops.GpuJoin
   0.0%   100.0%       0.038s       4.72e-04s     C       80       4   theano.sandbox.cuda.basic_ops.GpuIncSubtensor
   0.0%   100.0%       0.013s       1.34e-04s     C      100       5   theano.sandbox.cuda.dnn.GpuDnnConv
   0.0%   100.0%       0.011s       6.05e-06s     C     1880      94   theano.sandbox.cuda.basic_ops.GpuSubtensor
   0.0%   100.0%       0.010s       1.05e-04s     C      100       5   theano.sandbox.cuda.blas.GpuDot22
   0.0%   100.0%       0.008s       2.83e-06s     C     2660     133   theano.sandbox.cuda.basic_ops.GpuDimShuffle
   0.0%   100.0%       0.005s       1.20e-04s     C       40       2   theano.sandbox.cuda.basic_ops.GpuAlloc
   0.0%   100.0%       0.004s       7.28e-06s     C      520      26   theano.sandbox.cuda.basic_ops.GpuReshape
   0.0%   100.0%       0.002s       3.42e-06s     C      640      32   theano.tensor.elemwise.Elemwise
   0.0%   100.0%       0.002s       3.62e-05s     C       60       3   theano.sandbox.cuda.blas.GpuGemm
   0.0%   100.0%       0.002s       1.39e-05s     C      120       6   theano.sandbox.cuda.basic_ops.GpuContiguous
   0.0%   100.0%       0.001s       9.62e-06s     C      140       7   theano.sandbox.cuda.basic_ops.GpuAllocEmpty
   0.0%   100.0%       0.001s       2.73e-06s     C      340      17   theano.tensor.opt.MakeVector
   0.0%   100.0%       0.001s       7.41e-06s     Py     120       6   theano.compile.ops.Rebroadcast
   0.0%   100.0%       0.001s       2.34e-06s     C      320      16   theano.compile.ops.Shape_i
   0.0%   100.0%       0.000s       3.80e-06s     C      100       5   theano.sandbox.cuda.dnn.GpuDnnConvDesc
   0.0%   100.0%       0.000s       3.15e-06s     C       80       4   theano.tensor.basic.ScalarFromTensor
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  97.3%    97.3%     140.122s       7.01e+00s     Py      20        1   do_whileall_inplace,gpu,scan_fn}
   2.1%    99.4%       3.008s       8.35e-03s     C      360       18   GpuElemwise{mul,no_inplace}
   0.3%    99.7%       0.427s       1.78e-03s     C      240       12   GpuCAReduce{add}{0,1,0}
   0.2%    99.9%       0.228s       2.85e-04s     C      800       40   GpuJoin
   0.0%    99.9%       0.037s       9.17e-04s     C       40        2   GpuIncSubtensor{Set;::, int32:int32:}
   0.0%    99.9%       0.029s       1.79e-04s     C      160        8   GpuElemwise{Composite{maximum(i0, (i1 - Abs((i2 - i3))))},no_inplace}
   0.0%    99.9%       0.015s       2.32e-05s     C      660       33   GpuElemwise{maximum,no_inplace}
   0.0%    99.9%       0.013s       1.34e-04s     C      100        5   GpuDnnConv{algo='small', inplace=True}
   0.0%    99.9%       0.011s       2.32e-05s     C      480       24   GpuElemwise{minimum,no_inplace}
   0.0%   100.0%       0.010s       1.05e-04s     C      100        5   GpuDot22
   0.0%   100.0%       0.006s       4.35e-06s     C     1440       72   GpuSubtensor{::, int64}
   0.0%   100.0%       0.005s       3.03e-05s     C      160        8   GpuElemwise{Composite{(i0 + (i1 * i2 * i3))},no_inplace}
   0.0%   100.0%       0.005s       1.20e-04s     C       40        2   GpuAlloc
   0.0%   100.0%       0.004s       2.23e-05s     C      200       10   GpuElemwise{Minimum}[(0, 0)]
   0.0%   100.0%       0.004s       2.69e-05s     C      160        8   GpuElemwise{Composite{(i0 * (i1 + (i2 * (i3 + i4))))},no_inplace}
   0.0%   100.0%       0.004s       2.68e-05s     C      160        8   GpuElemwise{Composite{Abs((i0 - i1))},no_inplace}
   0.0%   100.0%       0.004s       2.35e-05s     C      160        8   GpuElemwise{Composite{((i0 * i1) - i2)},no_inplace}
   0.0%   100.0%       0.004s       2.61e-06s     C     1360       68   GpuDimShuffle{0,x}
   0.0%   100.0%       0.004s       8.80e-05s     C       40        2   GpuSubtensor{::, :int32:}
   0.0%   100.0%       0.003s       2.48e-05s     C      140        7   GpuElemwise{add,no_inplace}
   ... (remaining 55 Ops account for   0.02%(0.03s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  97.3%    97.3%     140.122s       7.01e+00s     20   548                     do_whileall_inplace,gpu,scan_fn}(TensorConstant{1000}, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, TensorConstant{1000}, GpuJoin.0, GpuDimShuffle{0,1,2,x}.0)
    input 0: dtype=int16, shape=(), strides=c 
    input 1: dtype=float32, shape=(1001, 16, 5776, 1), strides=c 
    input 2: dtype=float32, shape=(1001, 16, 1, 1), strides=c 
    input 3: dtype=int16, shape=(), strides=c 
    input 4: dtype=float32, shape=(16, 323, 1), strides=c 
    input 5: dtype=float32, shape=(16, 323, 5776, 1), strides=c 
    output 0: dtype=float32, shape=(1001, 16, 5776, 1), strides=c 
    output 1: dtype=float32, shape=(1001, 16, 1, 1), strides=c 
    output 2: dtype=float32, shape=(1000,), strides=c 
   0.6%    97.9%       0.806s       4.03e-02s     20   500                     GpuElemwise{mul,no_inplace}(<CudaNdarrayType(float32, (False, True, False, False, False, True))>, GpuElemwise{Composite{maximum(i0, (i1 - Abs((i2 - i3))))},no_inplace}.0)
    input 0: dtype=float32, shape=(16, 1, 1, 100, 100, 1), strides=(200000, 0, 0, 100, 1, 0) 
    input 1: dtype=float32, shape=(16, 17, 1, 1, 100, 100), strides=c 
    output 0: dtype=float32, shape=(16, 17, 1, 100, 100, 100), strides=c 
   0.4%    98.3%       0.647s       3.24e-02s     20   520                     GpuElemwise{mul,no_inplace}(GpuElemwise{Composite{maximum(i0, (i1 - Abs((i2 - i3))))},no_inplace}.0, GpuDimShuffle{0,1,2,x,3,4}.0)
    input 0: dtype=float32, shape=(16, 17, 1, 100, 100, 1), strides=c 
    input 1: dtype=float32, shape=(16, 17, 1, 1, 100, 100), strides=c 
    output 0: dtype=float32, shape=(16, 17, 1, 100, 100, 100), strides=c 
   0.3%    98.6%       0.435s       2.17e-02s     20   494                     GpuElemwise{mul,no_inplace}(<CudaNdarrayType(float32, (False, True, False, False, False, True))>, GpuElemwise{Composite{maximum(i0, (i1 - Abs((i2 - i3))))},no_inplace}.0)
    input 0: dtype=float32, shape=(16, 1, 1, 100, 100, 1), strides=(200000, 0, 0, 100, 1, 0) 
    input 1: dtype=float32, shape=(16, 9, 1, 1, 100, 100), strides=c 
    output 0: dtype=float32, shape=(16, 9, 1, 100, 100, 100), strides=c 
   0.3%    98.9%       0.384s       1.92e-02s     20   493                     GpuElemwise{mul,no_inplace}(<CudaNdarrayType(float32, (False, True, False, False, False, True))>, GpuElemwise{Composite{maximum(i0, (i1 - Abs((i2 - i3))))},no_inplace}.0)
    input 0: dtype=float32, shape=(16, 1, 1, 100, 100, 1), strides=(200000, 0, 0, 100, 1, 0) 
    input 1: dtype=float32, shape=(16, 8, 1, 1, 100, 100), strides=c 
    output 0: dtype=float32, shape=(16, 8, 1, 100, 100, 100), strides=c 
   0.2%    99.1%       0.345s       1.72e-02s     20   515                     GpuElemwise{mul,no_inplace}(GpuElemwise{Composite{maximum(i0, (i1 - Abs((i2 - i3))))},no_inplace}.0, GpuDimShuffle{0,1,2,x,3,4}.0)
    input 0: dtype=float32, shape=(16, 9, 1, 100, 100, 1), strides=c 
    input 1: dtype=float32, shape=(16, 9, 1, 1, 100, 100), strides=c 
    output 0: dtype=float32, shape=(16, 9, 1, 100, 100, 100), strides=c 
   0.2%    99.4%       0.306s       1.53e-02s     20   514                     GpuElemwise{mul,no_inplace}(GpuElemwise{Composite{maximum(i0, (i1 - Abs((i2 - i3))))},no_inplace}.0, GpuDimShuffle{0,1,2,x,3,4}.0)
    input 0: dtype=float32, shape=(16, 8, 1, 100, 100, 1), strides=c 
    input 1: dtype=float32, shape=(16, 8, 1, 1, 100, 100), strides=c 
    output 0: dtype=float32, shape=(16, 8, 1, 100, 100, 100), strides=c 
   0.1%    99.4%       0.098s       4.92e-03s     20   508                     GpuCAReduce{add}{0,1,0}(GpuReshape{3}.0)
    input 0: dtype=float32, shape=(27200, 100, 100), strides=c 
    output 0: dtype=float32, shape=(27200, 100), strides=c 
   0.1%    99.5%       0.098s       4.91e-03s     20   528                     GpuCAReduce{add}{0,1,0}(GpuReshape{3}.0)
    input 0: dtype=float32, shape=(27200, 100, 100), strides=c 
    output 0: dtype=float32, shape=(27200, 100), strides=c 
   0.0%    99.5%       0.067s       3.34e-03s     20   544                     GpuJoin(TensorConstant{1}, GpuSubtensor{::, :int32:}.0, GpuSubtensor{::, :int32:}.0)
    input 0: dtype=int8, shape=(), strides=c 
    input 1: dtype=float32, shape=(16, 171, 5776), strides=c 
    input 2: dtype=float32, shape=(16, 152, 5776), strides=c 
    output 0: dtype=float32, shape=(16, 323, 5776), strides=c 
   0.0%    99.6%       0.052s       2.61e-03s     20   503                     GpuCAReduce{add}{0,1,0}(GpuReshape{3}.0)
    input 0: dtype=float32, shape=(14400, 100, 100), strides=c 
    output 0: dtype=float32, shape=(14400, 100), strides=c 
   0.0%    99.6%       0.052s       2.58e-03s     20   523                     GpuCAReduce{add}{0,1,0}(GpuReshape{3}.0)
    input 0: dtype=float32, shape=(14400, 100, 100), strides=c 
    output 0: dtype=float32, shape=(14400, 100), strides=c 
   0.0%    99.6%       0.046s       2.31e-03s     20   502                     GpuCAReduce{add}{0,1,0}(GpuReshape{3}.0)
    input 0: dtype=float32, shape=(12800, 100, 100), strides=c 
    output 0: dtype=float32, shape=(12800, 100), strides=c 
   0.0%    99.7%       0.046s       2.30e-03s     20   522                     GpuCAReduce{add}{0,1,0}(GpuReshape{3}.0)
    input 0: dtype=float32, shape=(12800, 100, 100), strides=c 
    output 0: dtype=float32, shape=(12800, 100), strides=c 
   0.0%    99.7%       0.038s       1.92e-03s     20   266                     GpuElemwise{mul,no_inplace}(<CudaNdarrayType(float32, (False, True, False, False, False, True))>, GpuElemwise{Composite{maximum(i0, (i1 - Abs((i2 - i3))))},no_inplace}.0)
    input 0: dtype=float32, shape=(16, 1, 1, 100, 100, 1), strides=(200000, 0, 0, 100, 1, 0) 
    input 1: dtype=float32, shape=(16, 1, 1, 1, 100, 100), strides=c 
    output 0: dtype=float32, shape=(16, 1, 1, 100, 100, 100), strides=c 
   0.0%    99.7%       0.038s       1.92e-03s     20   437                     GpuElemwise{mul,no_inplace}(GpuElemwise{Composite{maximum(i0, (i1 - Abs((i2 - i3))))},no_inplace}.0, GpuDimShuffle{0,1,2,x,3,4}.0)
    input 0: dtype=float32, shape=(16, 1, 1, 100, 100, 1), strides=c 
    input 1: dtype=float32, shape=(16, 1, 1, 1, 100, 100), strides=c 
    output 0: dtype=float32, shape=(16, 1, 1, 100, 100, 100), strides=c 
   0.0%    99.7%       0.019s       9.63e-04s     20   539                     GpuIncSubtensor{Set;::, int32:int32:}(<CudaNdarrayType(float32, 3D)>, GpuReshape{3}.0, ScalarFromTensor.0, ScalarFromTensor.0)
    input 0: dtype=float32, shape=(16, 180, 5776), strides=c 
    input 1: dtype=float32, shape=(16, 9, 5776), strides=c 
    input 2: dtype=int32, shape=4, strides=c 
    input 3: dtype=int32, shape=4, strides=c 
    output 0: dtype=float32, shape=(16, 180, 5776), strides=c 
   0.0%    99.8%       0.017s       8.72e-04s     20   538                     GpuIncSubtensor{Set;::, int32:int32:}(<CudaNdarrayType(float32, 3D)>, GpuReshape{3}.0, ScalarFromTensor.0, ScalarFromTensor.0)
    input 0: dtype=float32, shape=(16, 160, 5776), strides=c 
    input 1: dtype=float32, shape=(16, 8, 5776), strides=c 
    input 2: dtype=int32, shape=4, strides=c 
    input 3: dtype=int32, shape=4, strides=c 
    output 0: dtype=float32, shape=(16, 160, 5776), strides=c 
   0.0%    99.8%       0.012s       5.93e-04s     20   549                     GpuCAReduce{add}{0,1,0}(GpuReshape{3}.0)
    input 0: dtype=float32, shape=(272, 5776, 1), strides=c 
    output 0: dtype=float32, shape=(272, 1), strides=c 
   0.0%    99.8%       0.009s       4.60e-04s     20   439                     GpuJoin(TensorConstant{1}, GpuDimShuffle{0,x,1}.0, GpuDimShuffle{0,x,1}.0, GpuDimShuffle{0,x,1}.0, GpuDimShuffle{0,x,1}.0, GpuDimShuffle{0,x,1}.0, GpuDimShuffle{0,x,1}.0, GpuDimShuffle{0,x,1}.0, GpuDimShuffle{0,x,1}.0, GpuDimShuffle{0,x,1}.0)
    input 0: dtype=int8, shape=(), strides=c 
    input 1: dtype=float32, shape=(16, 1, 4), strides=c 
    input 2: dtype=float32, shape=(16, 1, 4), strides=c 
    input 3: dtype=float32, shape=(16, 1, 4), strides=c 
    input 4: dtype=float32, shape=(16, 1, 4), strides=c 
    input 5: dtype=float32, shape=(16, 1, 4), strides=c 
    input 6: dtype=float32, shape=(16, 1, 4), strides=c 
    input 7: dtype=float32, shape=(16, 1, 4), strides=c 
    input 8: dtype=float32, shape=(16, 1, 4), strides=c 
    input 9: dtype=float32, shape=(16, 1, 4), strides=c 
    output 0: dtype=float32, shape=(16, 9, 4), strides=c 
   ... (remaining 543 Apply instances account for 0.23%(0.33s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max peak memory with current setting
        CPU: 0KB (0KB)
        GPU: 1451314KB (2530966KB)
        CPU + GPU: 1451314KB (2530967KB)
    Max peak memory with current setting and Theano flag optimizer_excluding=inplace
        CPU: 0KB (0KB)
        GPU: 1451314KB (2530966KB)
        CPU + GPU: 1451314KB (2530967KB)
    Max peak memory if allow_gc=False (linker don't make a difference)
        CPU: 1KB
        GPU: 5084063KB
        CPU + GPU: 5084064KB
---

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

     1088000000B  [(16, 17, 1, 100, 100, 100)] c GpuElemwise{mul,no_inplace}(GpuElemwise{Composite{maximum(i0, (i1 - Abs((i2 - i3))))},no_inplace}.0, GpuDimShuffle{0,1,2,x,3,4}.0)
     1088000000B  [(27200, 100, 100)] v GpuReshape{3}(GpuElemwise{mul,no_inplace}.0, MakeVector{dtype='int64'}.0)
     1088000000B  [(27200, 100, 100)] v GpuReshape{3}(GpuElemwise{mul,no_inplace}.0, MakeVector{dtype='int64'}.0)
     1088000000B  [(16, 17, 1, 100, 100, 100)] c GpuElemwise{mul,no_inplace}(<CudaNdarrayType(float32, (False, True, False, False, False, True))>, GpuElemwise{Composite{maximum(i0, (i1 - Abs((i2 - i3))))},no_inplace}.0)
     576000000B  [(14400, 100, 100)] v GpuReshape{3}(GpuElemwise{mul,no_inplace}.0, MakeVector{dtype='int64'}.0)
     576000000B  [(14400, 100, 100)] v GpuReshape{3}(GpuElemwise{mul,no_inplace}.0, MakeVector{dtype='int64'}.0)
     576000000B  [(16, 9, 1, 100, 100, 100)] c GpuElemwise{mul,no_inplace}(GpuElemwise{Composite{maximum(i0, (i1 - Abs((i2 - i3))))},no_inplace}.0, GpuDimShuffle{0,1,2,x,3,4}.0)
     576000000B  [(16, 9, 1, 100, 100, 100)] c GpuElemwise{mul,no_inplace}(<CudaNdarrayType(float32, (False, True, False, False, False, True))>, GpuElemwise{Composite{maximum(i0, (i1 - Abs((i2 - i3))))},no_inplace}.0)
     512000000B  [(12800, 100, 100)] v GpuReshape{3}(GpuElemwise{mul,no_inplace}.0, MakeVector{dtype='int64'}.0)
     512000000B  [(12800, 100, 100)] v GpuReshape{3}(GpuElemwise{mul,no_inplace}.0, MakeVector{dtype='int64'}.0)
     512000000B  [(16, 8, 1, 100, 100, 100)] c GpuElemwise{mul,no_inplace}(GpuElemwise{Composite{maximum(i0, (i1 - Abs((i2 - i3))))},no_inplace}.0, GpuDimShuffle{0,1,2,x,3,4}.0)
     512000000B  [(16, 8, 1, 100, 100, 100)] c GpuElemwise{mul,no_inplace}(<CudaNdarrayType(float32, (False, True, False, False, False, True))>, GpuElemwise{Composite{maximum(i0, (i1 - Abs((i2 - i3))))},no_inplace}.0)
     370101728B  [(1001, 16, 5776, 1), (1001, 16, 1, 1), (1000,)] i i c do_whileall_inplace,gpu,scan_fn}(TensorConstant{1000}, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, TensorConstant{1000}, GpuJoin.0, GpuDimShuffle{0,1,2,x}.0)
     370033664B  [(1001, 16, 5776, 1)] c GpuAllocEmpty(TensorConstant{1001}, Shape_i{0}.0, Shape_i{1}.0, Shape_i{2}.0)
     370033664B  [(1001, 16, 5776, 1)] i GpuIncSubtensor{InplaceSet;:int64:}(GpuAllocEmpty.0, Rebroadcast{0}.0, Constant{1})
     369664000B  [(1000, 16, 5776, 1)] v GpuSubtensor{int64::}(do_whileall_inplace,gpu,scan_fn}.0, Constant{1})
     119401472B  [(16, 323, 5776, 1)] v GpuDimShuffle{0,1,2,x}(GpuJoin.0)
     119401472B  [(16, 323, 5776)] c GpuJoin(TensorConstant{1}, GpuSubtensor{::, :int32:}.0, GpuSubtensor{::, :int32:}.0)
      66539520B  [(16, 180, 5776)] c GpuIncSubtensor{Set;::, int32:int32:}(<CudaNdarrayType(float32, 3D)>, GpuReshape{3}.0, ScalarFromTensor.0, ScalarFromTensor.0)
      64000000B  [(1600, 100, 100)] v GpuReshape{3}(GpuElemwise{mul,no_inplace}.0, MakeVector{dtype='int64'}.0)
   ... (remaining 543 Apply account for 613363292B/11166538812B ((5.49%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  Sorry, no tip for today.

Scan Op profiling ( scan_fn )
==================
  Message: None
  Time in 20 calls of the op (for a total of 20000 steps) 1.401082e+02s

  Total time spent in calling the VM 1.388469e+02s (99.100%)
  Total overhead (computing slices..) 1.261342e+00s (0.900%)

Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  50.3%    50.3%      61.893s       3.44e-04s     C   180000       9   theano.sandbox.cuda.basic_ops.GpuElemwise
  47.0%    97.2%      57.816s       7.23e-04s     C    80000       4   theano.sandbox.cuda.basic_ops.GpuCAReduce
   0.7%    97.9%       0.826s       4.13e-05s     C    20000       1   theano.sandbox.cuda.basic_ops.GpuFromHost
   0.6%    98.5%       0.705s       5.88e-06s     C   120000       6   theano.sandbox.cuda.basic_ops.GpuDimShuffle
   0.4%    98.9%       0.464s       2.32e-05s     C    20000       1   theano.sandbox.cuda.basic_ops.HostFromGpu
   0.3%    99.1%       0.346s       4.33e-06s     C    80000       4   theano.sandbox.cuda.basic_ops.GpuReshape
   0.2%    99.4%       0.279s       6.98e-06s     Py   40000       2   theano.compile.ops.Rebroadcast
   0.2%    99.6%       0.223s       2.79e-06s     C    80000       4   theano.tensor.elemwise.Elemwise
   0.2%    99.7%       0.208s       2.60e-06s     C    80000       4   theano.compile.ops.Shape_i
   0.1%    99.9%       0.177s       2.96e-06s     C    60000       3   theano.sandbox.cuda.basic_ops.GpuSubtensor
   0.1%   100.0%       0.168s       2.10e-06s     C    80000       4   theano.tensor.opt.MakeVector
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  47.2%    47.2%      58.100s       1.45e-03s     C     40000        2   GpuElemwise{mul,no_inplace}
  46.6%    93.8%      57.350s       9.56e-04s     C     60000        3   GpuCAReduce{add}{0,1,0}
   1.1%    94.9%       1.411s       2.35e-05s     C     60000        3   GpuElemwise{sub,no_inplace}
   0.8%    95.7%       0.931s       4.65e-05s     C     20000        1   GpuElemwise{Composite{(((i0 / i1) / i2) / i3)},no_inplace}
   0.7%    96.4%       0.826s       4.13e-05s     C     20000        1   GpuFromHost
   0.4%    96.8%       0.534s       8.89e-06s     C     60000        3   GpuDimShuffle{x,x,x}
   0.4%    97.2%       0.517s       2.58e-05s     C     20000        1   GpuElemwise{Composite{(((inv(i0) / i1) / i2) * i3)},no_inplace}
   0.4%    97.6%       0.507s       2.53e-05s     C     20000        1   GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)]
   0.4%    98.0%       0.466s       2.33e-05s     C     20000        1   GpuCAReduce{pre=sqr,red=add}{1,1,1}
   0.4%    98.4%       0.464s       2.32e-05s     C     20000        1   HostFromGpu
   0.3%    98.7%       0.428s       2.14e-05s     C     20000        1   GpuElemwise{Composite{(((i0 * i1) * i2) * (i3 - i2))}}[(0, 1)]
   0.3%    99.0%       0.346s       4.33e-06s     C     80000        4   GpuReshape{3}
   0.1%    99.1%       0.177s       2.96e-06s     C     60000        3   GpuSubtensor{int64}
   0.1%    99.3%       0.168s       2.10e-06s     C     80000        4   MakeVector{dtype='int64'}
   0.1%    99.4%       0.151s       7.57e-06s     Py    20000        1   Rebroadcast{?,1}
   0.1%    99.5%       0.128s       6.38e-06s     Py    20000        1   Rebroadcast{?,0}
   0.1%    99.6%       0.121s       3.03e-06s     C     40000        2   Shape_i{2}
   0.1%    99.7%       0.062s       3.12e-06s     C     20000        1   Elemwise{Cast{float32}}
   0.1%    99.7%       0.062s       3.08e-06s     C     20000        1   GpuDimShuffle{0,1,x,2}
   0.0%    99.8%       0.061s       3.03e-06s     C     20000        1   Elemwise{lt,no_inplace}
   ... (remaining 6 Ops account for   0.24%(0.30s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Mflops> <Gflops/s> <Apply name>
  42.1%    42.1%      51.780s       2.59e-03s   20000    22                     GpuCAReduce{add}{0,1,0}(GpuReshape{3}.0)
    input 0: dtype=float32, shape=(5168, 5776, 1), strides=c 
    output 0: dtype=float32, shape=(5168, 1), strides=c 
  26.2%    68.2%      32.217s       1.61e-03s   20000     6                     GpuElemwise{mul,no_inplace}(<CudaNdarrayType(float32, (False, False, False, True))>, GpuDimShuffle{0,x,1,2}.0)
    input 0: dtype=float32, shape=(16, 323, 5776, 1), strides=c 
    input 1: dtype=float32, shape=(16, 1, 5776, 1), strides=c 
    output 0: dtype=float32, shape=(16, 323, 5776, 1), strides=c 
  21.0%    89.3%      25.883s       1.29e-03s   20000    34                     GpuElemwise{mul,no_inplace}(GpuDimShuffle{0,1,x,2}.0, <CudaNdarrayType(float32, (False, False, False, True))>)
    input 0: dtype=float32, shape=(16, 323, 1, 1), strides=c 
    input 1: dtype=float32, shape=(16, 323, 5776, 1), strides=c 
    output 0: dtype=float32, shape=(16, 323, 5776, 1), strides=c 
   4.0%    93.3%       4.939s       2.47e-04s   20000    39                     GpuCAReduce{add}{0,1,0}(GpuReshape{3}.0)
    input 0: dtype=float32, shape=(16, 323, 5776), strides=c 
    output 0: dtype=float32, shape=(16, 5776), strides=c 
   0.8%    94.0%       0.931s       4.65e-05s   20000    29                     GpuElemwise{Composite{(((i0 / i1) / i2) / i3)},no_inplace}(GpuCAReduce{pre=sqr,red=add}{1,1,1}.0, GpuSubtensor{int64}.0, GpuSubtensor{int64}.0, GpuSubtensor{int64}.0)
    input 0: dtype=float32, shape=(), strides=c 
    input 1: dtype=float32, shape=(), strides=c 
    input 2: dtype=float32, shape=(), strides=c 
    input 3: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.7%    94.7%       0.826s       4.13e-05s   20000    13                     GpuFromHost(Elemwise{Cast{float32}}.0)
    input 0: dtype=float32, shape=(3,), strides=c 
    output 0: dtype=float32, shape=(3,), strides=c 
   0.5%    95.2%       0.631s       3.16e-05s   20000    30                     GpuCAReduce{add}{0,1,0}(GpuElemwise{Composite{(((i0 * i1) * i2) * (i3 - i2))}}[(0, 1)].0)
    input 0: dtype=float32, shape=(16, 323, 1), strides=c 
    output 0: dtype=float32, shape=(16, 1), strides=c 
   0.4%    95.7%       0.546s       2.73e-05s   20000    41                     GpuElemwise{sub,no_inplace}(<CudaNdarrayType(float32, 3D)>, GpuReshape{3}.0)
    input 0: dtype=float32, shape=(16, 5776, 1), strides=c 
    input 1: dtype=float32, shape=(16, 5776, 1), strides=c 
    output 0: dtype=float32, shape=(16, 5776, 1), strides=c 
   0.4%    96.1%       0.517s       2.58e-05s   20000    23                     GpuElemwise{Composite{(((inv(i0) / i1) / i2) * i3)},no_inplace}(GpuDimShuffle{x,x,x}.0, GpuDimShuffle{x,x,x}.0, GpuDimShuffle{x,x,x}.0, CudaNdarrayConstant{[[[ 2.]]]})
    input 0: dtype=float32, shape=(1, 1, 1), strides=c 
    input 1: dtype=float32, shape=(1, 1, 1), strides=c 
    input 2: dtype=float32, shape=(1, 1, 1), strides=c 
    input 3: dtype=float32, shape=(1, 1, 1), strides=c 
    output 0: dtype=float32, shape=(1, 1, 1), strides=c 
   0.4%    96.5%       0.507s       2.53e-05s   20000    25                     GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)](GpuReshape{3}.0, Rebroadcast{?,1}.0)
    input 0: dtype=float32, shape=(16, 323, 1), strides=c 
    input 1: dtype=float32, shape=(16, 1, 1), strides=c 
    output 0: dtype=float32, shape=(16, 323, 1), strides=c 
   0.4%    96.9%       0.466s       2.33e-05s   20000    27                     GpuCAReduce{pre=sqr,red=add}{1,1,1}(GpuElemwise{sub,no_inplace}.0)
    input 0: dtype=float32, shape=(16, 323, 1), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.4%    97.2%       0.464s       2.32e-05s   20000    32                     HostFromGpu(GpuElemwise{Composite{(((i0 / i1) / i2) / i3)},no_inplace}.0)
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(), strides=c 
   0.4%    97.6%       0.451s       2.25e-05s   20000    38                     GpuElemwise{sub,no_inplace}(<CudaNdarrayType(float32, 3D)>, Rebroadcast{?,0}.0)
    input 0: dtype=float32, shape=(16, 1, 1), strides=c 
    input 1: dtype=float32, shape=(16, 1, 1), strides=c 
    output 0: dtype=float32, shape=(16, 1, 1), strides=c 
   0.4%    98.0%       0.434s       2.17e-05s   20000    19                     GpuDimShuffle{x,x,x}(GpuSubtensor{int64}.0)
    input 0: dtype=float32, shape=(), strides=c 
    output 0: dtype=float32, shape=(1, 1, 1), strides=c 
   0.3%    98.3%       0.428s       2.14e-05s   20000    28                     GpuElemwise{Composite{(((i0 * i1) * i2) * (i3 - i2))}}[(0, 1)](GpuElemwise{Composite{(((inv(i0) / i1) / i2) * i3)},no_inplace}.0, GpuElemwise{sub,no_inplace}.0, GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, CudaNdarrayConstant{[[[ 1.]]]})
    input 0: dtype=float32, shape=(1, 1, 1), strides=c 
    input 1: dtype=float32, shape=(16, 323, 1), strides=c 
    input 2: dtype=float32, shape=(16, 323, 1), strides=c 
    input 3: dtype=float32, shape=(1, 1, 1), strides=c 
    output 0: dtype=float32, shape=(16, 323, 1), strides=c 
   0.3%    98.6%       0.413s       2.07e-05s   20000    26                     GpuElemwise{sub,no_inplace}(GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, <CudaNdarrayType(float32, (False, False, True))>)
    input 0: dtype=float32, shape=(16, 323, 1), strides=c 
    input 1: dtype=float32, shape=(16, 323, 1), strides=c 
    output 0: dtype=float32, shape=(16, 323, 1), strides=c 
   0.1%    98.8%       0.151s       7.57e-06s   20000     5                     Rebroadcast{?,1}(<CudaNdarrayType(float32, 3D)>)
    input 0: dtype=float32, shape=(16, 1, 1), strides=c 
    output 0: dtype=float32, shape=(16, 1, 1), strides=c 
   0.1%    98.9%       0.128s       6.38e-06s   20000    36                     Rebroadcast{?,0}(GpuDimShuffle{0,x,1}.0)
    input 0: dtype=float32, shape=(16, 1, 1), strides=c 
    output 0: dtype=float32, shape=(16, 1, 1), strides=c 
   0.1%    98.9%       0.093s       4.64e-06s   20000    18                     GpuReshape{3}(GpuElemwise{mul,no_inplace}.0, MakeVector{dtype='int64'}.0)
    input 0: dtype=float32, shape=(16, 323, 5776, 1), strides=c 
    input 1: dtype=int64, shape=(3,), strides=c 
    output 0: dtype=float32, shape=(5168, 5776, 1), strides=c 
   0.1%    99.0%       0.090s       4.52e-06s   20000    37                     GpuReshape{3}(GpuElemwise{mul,no_inplace}.0, MakeVector{dtype='int64'}.0)
    input 0: dtype=float32, shape=(16, 323, 5776, 1), strides=c 
    input 1: dtype=int64, shape=(3,), strides=c 
    output 0: dtype=float32, shape=(16, 323, 5776), strides=c 
   ... (remaining 22 Apply instances account for 0.98%(1.21s) of the runtime)

Memory Profile
(Sparse variables are ignored)
(For values in brackets, it's for linker = c|py
---
    Max peak memory with current setting
        CPU: 0KB (0KB)
        GPU: 116964KB (116964KB)
        CPU + GPU: 116964KB (116964KB)
    Max peak memory with current setting and Theano flag optimizer_excluding=inplace
        CPU: 0KB (0KB)
        GPU: 116964KB (116964KB)
        CPU + GPU: 116964KB (116964KB)
    Max peak memory if allow_gc=False (linker don't make a difference)
        CPU: 0KB
        GPU: 233969KB
        CPU + GPU: 233969KB
---

    <Sum apply outputs (bytes)> <Apply outputs shape> <created/inplace/view> <Apply node>

     119401472B  [(16, 323, 5776, 1)] c GpuElemwise{mul,no_inplace}(<CudaNdarrayType(float32, (False, False, False, True))>, GpuDimShuffle{0,x,1,2}.0)
     119401472B  [(5168, 5776, 1)] v GpuReshape{3}(GpuElemwise{mul,no_inplace}.0, MakeVector{dtype='int64'}.0)
     119401472B  [(16, 323, 5776)] v GpuReshape{3}(GpuElemwise{mul,no_inplace}.0, MakeVector{dtype='int64'}.0)
     119401472B  [(16, 323, 5776, 1)] c GpuElemwise{mul,no_inplace}(GpuDimShuffle{0,1,x,2}.0, <CudaNdarrayType(float32, (False, False, False, True))>)
        369664B  [(16, 1, 5776, 1)] v GpuDimShuffle{0,x,1,2}(<CudaNdarrayType(float32, 3D)>)
        369664B  [(16, 5776)] c GpuCAReduce{add}{0,1,0}(GpuReshape{3}.0)
        369664B  [(16, 5776, 1)] c GpuElemwise{sub,no_inplace}(<CudaNdarrayType(float32, 3D)>, GpuReshape{3}.0)
        369664B  [(16, 5776, 1)] v GpuReshape{3}(GpuCAReduce{add}{0,1,0}.0, MakeVector{dtype='int64'}.0)
         20672B  [(16, 323, 1)] i GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)](GpuReshape{3}.0, Rebroadcast{?,1}.0)
         20672B  [(16, 323, 1)] i GpuElemwise{Composite{(((i0 * i1) * i2) * (i3 - i2))}}[(0, 1)](GpuElemwise{Composite{(((inv(i0) / i1) / i2) * i3)},no_inplace}.0, GpuElemwise{sub,no_inplace}.0, GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, CudaNdarrayConstant{[[[ 1.]]]})
         20672B  [(16, 323, 1)] v GpuReshape{3}(GpuCAReduce{add}{0,1,0}.0, MakeVector{dtype='int64'}.0)
         20672B  [(16, 323, 1, 1)] v GpuDimShuffle{0,1,x,2}(GpuElemwise{Composite{(((i0 * i1) * i2) * (i3 - i2))}}[(0, 1)].0)
         20672B  [(5168, 1)] c GpuCAReduce{add}{0,1,0}(GpuReshape{3}.0)
         20672B  [(16, 323, 1)] c GpuElemwise{sub,no_inplace}(GpuElemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0, <CudaNdarrayType(float32, (False, False, True))>)
   ... (remaining 28 Apply account for  529B/479209105B ((0.00%)) of the Apply with dense outputs sizes)

    <created/inplace/view> is taken from the Op's declaration.
    Apply nodes marked 'inplace' or 'view' may actually allocate memory, this is not reported here. If you use DebugMode, warnings will be emitted in those cases.

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  Sorry, no tip for today.
Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  Sorry, no tip for today.
Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  Sorry, no tip for today.
Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  Sorry, no tip for today.
Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  Sorry, no tip for today.
Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  Sorry, no tip for today.
Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  Sorry, no tip for today.
Function profiling
==================
  Message: recurrent_mlp_gpu.py:494
  Time in 0 calls to Function.__call__: 0.000000e+00s
  Total compile time: 2.596068e-01s
    Number of Apply nodes: 19
    Theano Optimizer time: 2.086930e-01s
       Theano validate time: 6.520748e-03s
    Theano Linker time (includes C, CUDA code generation/compiling): 2.601600e-02s
       Import time 3.092289e-03s

Time in all call to theano.grad() 1.141292e+01s
Time since theano import 430.495s
Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  Sorry, no tip for today.
Function profiling
==================
  Message: recurrent_mlp_gpu.py:499
  Time in 0 calls to Function.__call__: 0.000000e+00s
  Total compile time: 3.853540e-01s
    Number of Apply nodes: 21
    Theano Optimizer time: 3.171949e-01s
       Theano validate time: 3.005505e-03s
    Theano Linker time (includes C, CUDA code generation/compiling): 4.248881e-02s
       Import time 5.197287e-03s

Time in all call to theano.grad() 1.141292e+01s
Time since theano import 430.496s
Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  Sorry, no tip for today.
Function profiling
==================
  Message: Sum of all(3) printed profiles at exit excluding Scan op profile.
  Time in 0 calls to Function.__call__: 0.000000e+00s
  Total compile time: 2.599997e+02s
    Number of Apply nodes: 473
    Theano Optimizer time: 1.590433e+02s
       Theano validate time: 2.293154e+00s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.008148e+02s
       Import time 3.118281e-01s

Time in all call to theano.grad() 1.141292e+01s
Time since theano import 430.496s
Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  Sorry, no tip for today.
