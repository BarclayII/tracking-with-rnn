Using gpu device 1: GeForce GTX TITAN X (CNMeM is disabled, CuDNN 3007)
Using CUDNN instead of Theano conv2d
5776
Initializing parameters
Building network
float32
float32
Building optimizer
Generating dataset
START
/usr/local/lib/python2.7/dist-packages/scipy/ndimage/interpolation.py:549: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.
  "the returned array has changed.", UserWarning)
Batch generation: 0.291475057602
Traceback (most recent call last):
  File "recurrent_mlp_gpu.py", line 524, in <module>
    cost, bbox_seq, att_seq, mask = train(_len, data[:, :], label[:, 0, :], att, label[:, :])
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/compile/function_module.py", line 871, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gof/link.py", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/scan_module/scan_op.py", line 951, in rval
    r = p(n, [x[0] for x in i], o)
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/scan_module/scan_op.py", line 940, in <lambda>
    self, node)
  File "theano/scan_module/scan_perform.pyx", line 405, in theano.scan_module.scan_perform.perform (/home/gq/.theano/compiledir_Linux-3.16--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/scan_perform/mod.cpp:4316)
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gof/link.py", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "theano/scan_module/scan_perform.pyx", line 397, in theano.scan_module.scan_perform.perform (/home/gq/.theano/compiledir_Linux-3.16--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/scan_perform/mod.cpp:4193)
MemoryError: alloc failed
Apply node that caused the error: Alloc(TensorConstant{0.0}, Elemwise{add,no_inplace}.0, Subtensor{int64}.0, Subtensor{int64}.0, Subtensor{int64}.0)
Toposort index: 6100
Inputs types: [TensorType(float32, scalar), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]
Inputs shapes: [(), (), (), (), ()]
Inputs strides: [(), (), (), (), ()]
Inputs values: [array(0.0, dtype=float32), array(1001), array(16), array(323), array(5776)]
Outputs clients: [[for{cpu,grad_of_scan_fn}(Subtensor{int64}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Alloc.0, Subtensor{::int64}.0, Alloc.0, Alloc.0, Join.0, Join.0)]]

Backtrace when the node is created(use Theano flag traceback.limit=N to make it longer):
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gradient.py", line 973, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gradient.py", line 973, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gradient.py", line 973, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gradient.py", line 1113, in access_term_cache
    input_grads = node.op.grad(inputs, new_output_grads)

HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
Apply node that caused the error: for{cpu,grad_of_scan_fn}(Elemwise{sub,no_inplace}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{:int64:}.0, Subtensor{::int64}.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Alloc.0, Subtensor{int64}.0, HostFromGpu.0, HostFromGpu.0, HostFromGpu.0, HostFromGpu.0, HostFromGpu.0, HostFromGpu.0, HostFromGpu.0, HostFromGpu.0, HostFromGpu.0, HostFromGpu.0, HostFromGpu.0, HostFromGpu.0, HostFromGpu.0, HostFromGpu.0)
Toposort index: 534
Inputs types: [TensorType(int64, scalar), TensorType(float32, 5D), TensorType(float32, 3D), TensorType(float32, 3D), TensorType(float32, 3D), TensorType(float32, 4D), TensorType(float32, 3D), TensorType(float32, 4D), TensorType(float32, 4D), TensorType(float32, 4D), TensorType(float32, 4D), TensorType(int32, vector), TensorType(float32, 3D), TensorType(float32, 3D), TensorType(float32, 3D), TensorType(float32, 4D), TensorType(float32, 3D), TensorType(float32, 4D), TensorType(float32, 4D), TensorType(float32, 4D), TensorType(float32, 4D), TensorType(int32, vector), TensorType(float32, 3D), TensorType(float32, 3D), TensorType(float32, 3D), TensorType(float32, 4D), TensorType(float32, 3D), TensorType(float32, 4D), TensorType(float32, 4D), TensorType(float32, 4D), TensorType(float32, 4D), TensorType(float32, vector), TensorType(float32, 5D), TensorType(float32, 3D), TensorType(float32, 3D), TensorType(float32, matrix), TensorType(float32, 3D), TensorType(float32, 3D), TensorType(float32, 3D), TensorType(float32, matrix), TensorType(float32, 3D), TensorType(float32, matrix), TensorType(float32, 3D), TensorType(float32, matrix), TensorType(float32, 3D), TensorType(float32, matrix), TensorType(int64, scalar), TensorType(float32, 4D), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, vector), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, vector), TensorType(float32, matrix), TensorType(float32, vector), TensorType(float32, matrix), TensorType(float32, vector), TensorType(float32, matrix), TensorType(float32, vector)]
Inputs shapes: [(), (20, 16, 1, 100, 100), (20, 16, 4), (20, 16, 3), (20, 16, 200), (20, 16, 1, 1), (20, 16, 4), (20, 16, 5776, 1), (20, 16, 1, 1), (20, 16, 180, 5776), (20, 16, 160, 5776), (20,), (20, 16, 4), (20, 16, 3), (20, 16, 200), (20, 16, 1, 1), (20, 16, 4), (20, 16, 5776, 1), (20, 16, 1, 1), (20, 16, 180, 5776), (20, 16, 160, 5776), (20,), (21, 16, 4), (21, 16, 3), (21, 16, 200), (21, 16, 1, 1), (21, 16, 4), (21, 16, 5776, 1), (21, 16, 1, 1), (21, 16, 180, 5776), (21, 16, 160, 5776), (21,), (21, 16, 1, 10, 10), (21, 5785, 200), (21, 200, 200), (21, 200), (21, 5785, 200), (21, 5785, 200), (21, 200, 200), (21, 200), (21, 200, 200), (21, 200), (21, 200, 4), (21, 4), (21, 200, 3), (21, 3), (), (16, 1, 10, 10), (5785, 200), (200, 200), (200,), (5785, 200), (5785, 200), (200, 200), (200,), (200, 200), (200,), (200, 4), (4,), (200, 3), (3,)]
Inputs strides: [(), (-40000, 800000, 40000, 400, 4), (-256, 16, 4), (-192, 12, 4), (-12800, 800, 4), (-64, 4, 4, 4), (-256, 16, 4), (-369664, 23104, 4, 4), (-64, 4, 4, 4), (-66539520, 4158720, 23104, 4), (-59146240, 3696640, 23104, 4), (-4,), (-256, 16, 4), (-192, 12, 4), (-12800, 800, 4), (-64, 4, 4, 4), (-256, 16, 4), (-369664, 23104, 4, 4), (-64, 4, 4, 4), (-66539520, 4158720, 23104, 4), (-59146240, 3696640, 23104, 4), (-4,), (-256, 16, 4), (192, 12, 4), (12800, 800, 4), (64, 4, 4, 4), (256, 16, 4), (369664, 23104, 4, 4), (64, 4, 4, 4), (66539520, 4158720, 23104, 4), (59146240, 3696640, 23104, 4), (4,), (6400, 400, 400, 40, 4), (4628000, 800, 4), (160000, 800, 4), (800, 4), (4628000, 800, 4), (4628000, 800, 4), (160000, 800, 4), (800, 4), (160000, 800, 4), (800, 4), (3200, 16, 4), (16, 4), (2400, 12, 4), (12, 4), (), (400, 400, 40, 4), (800, 4), (800, 4), (4,), (800, 4), (800, 4), (800, 4), (4,), (800, 4), (4,), (16, 4), (4,), (12, 4), (4,)]
Inputs values: [array(20), 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', array(20), 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', array([ 0.,  0.,  0.,  0.], dtype=float32), 'not shown', array([ 0.,  0.,  0.], dtype=float32)]
Outputs clients: [[], [], [], [], [], [], [], [], [], [], [Subtensor{int64}(for{cpu,grad_of_scan_fn}.10, Constant{-1})], [Subtensor{int64}(for{cpu,grad_of_scan_fn}.11, Constant{-1})], [Subtensor{int64}(for{cpu,grad_of_scan_fn}.12, Constant{-1})], [Subtensor{int64}(for{cpu,grad_of_scan_fn}.13, Constant{-1})], [Subtensor{int64}(for{cpu,grad_of_scan_fn}.14, Constant{-1})], [Subtensor{int64}(for{cpu,grad_of_scan_fn}.15, Constant{-1})], [Subtensor{int64}(for{cpu,grad_of_scan_fn}.16, Constant{-1})], [Subtensor{int64}(for{cpu,grad_of_scan_fn}.17, Constant{-1})], [Subtensor{int64}(for{cpu,grad_of_scan_fn}.18, Constant{-1})], [Subtensor{int64}(for{cpu,grad_of_scan_fn}.19, Constant{-1})], [Subtensor{int64}(for{cpu,grad_of_scan_fn}.20, Constant{-1})], [Subtensor{int64}(for{cpu,grad_of_scan_fn}.21, Constant{-1})], [], [], []]

Backtrace when the node is created(use Theano flag traceback.limit=N to make it longer):
  File "recurrent_mlp_gpu.py", line 470, in <module>
    train = T.function([seq_len_scalar, imgs, starts, startAtt, targets], [cost, bbox_seq, att_seq, mask_seq], updates=rmsprop(cost, params) if not test else None, allow_input_downcast=True)
  File "recurrent_mlp_gpu.py", line 458, in rmsprop
    grads = T.grad(cost, params, disconnected_inputs='ignore')
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gradient.py", line 561, in grad
    grad_dict, wrt, cost_name)
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gradient.py", line 1324, in _populate_grad_dict
    rval = [access_grad_cache(elem) for elem in wrt]
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gradient.py", line 973, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/gradient.py", line 1113, in access_term_cache
    input_grads = node.op.grad(inputs, new_output_grads)

HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
